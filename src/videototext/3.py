import os
import io
import time
import numpy as np
import matplotlib.pyplot as plt
from pydub import AudioSegment
from google.cloud import speech_v1p1beta1 as speech
from google.cloud import storage 
import torch
import torchaudio
from scipy.ndimage.measurements import label as scipy_label 


# =====================================================
# âœ… ì„¤ì • (ì†ë„ ìµœì í™” ì ìš©)
# =====================================================
INPUT_AUDIO_FILE = "match_audio.wav"
OUTPUT_DIR = "highlight_v2"

# âš ï¸ ì‹¤ì œ ì‚¬ìš©ìì˜ ë²„í‚· ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”!
GCS_BUCKET_NAME = "shortsgenie-voice" 

os.makedirs(OUTPUT_DIR, exist_ok=True)


# =====================================================
# âœ… ê³µìš© ìœ í‹¸: ì²˜ë¦¬ì‹œê°„ ì¸¡ì • & íƒ€ì„ í¬ë§·
# =====================================================
def timed(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"âœ… {func.__name__} ì™„ë£Œ (ê±¸ë¦° ì‹œê°„: {end - start:.2f}ì´ˆ)\n")
        return result
    return wrapper

def format_time(seconds):
    """ì´ˆë¥¼ HH:MM:SS.msms í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    ms = int((seconds - int(seconds)) * 100)
    s = int(seconds)
    m = s // 60
    h = m // 60
    return f"{h:02d}:{m % 60:02d}:{s % 60:02d}.{ms:02d}"


# =====================================================
# ğŸ’¡ GCS ìœ í‹¸ë¦¬í‹°
# =====================================================
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """ë¡œì»¬ íŒŒì¼ì„ GCSì— ì—…ë¡œë“œ"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    return f"gs://{bucket_name}/{destination_blob_name}"


# =====================================================
# ğŸ’¡ Silero VAD ìœ í‹¸ë¦¬í‹°
# =====================================================
@timed
def load_silero_vad_model():
    """Silero VAD ëª¨ë¸ ë¡œë“œ"""
    print("ğŸ§  Silero VAD ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )
        (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
        return model, get_speech_timestamps, read_audio
    except Exception as e:
        print(f"âŒ Silero VAD ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì˜¤ë¥˜: {e}")
        return None, None, None


# =====================================================
# [1/5] ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
# =====================================================
@timed
def preprocess_audio(input_path):
    print("ğŸ§ [1/5] ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì¤‘...")
    audio = AudioSegment.from_file(input_path)
    original_len = len(audio) / 1000

    cleaned = audio.set_frame_rate(16000).set_channels(1)
    cleaned = cleaned.apply_gain(-20 - cleaned.dBFS) 

    output_path = os.path.join(OUTPUT_DIR, "cleaned_audio.wav")
    cleaned.export(output_path, format="wav", codec="pcm_s16le")

    print(f"ğŸ“¦ ì›ë³¸ê¸¸ì´: {original_len:.1f}s â†’ ì „ì²˜ë¦¬ í›„: {len(cleaned)/1000:.1f}s\n")
    return output_path


# =====================================================
# [2/5] Silero VAD ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ íƒì§€ (VAD ê·¹ë‹¨ì  ìƒí–¥)
# =====================================================
@timed
def detect_highlight_candidates_vad(audio_path, model, get_speech_timestamps, read_audio, 
                                    vad_threshold=0.95, min_clip_duration=3.0): # ğŸ’¡ ì„ê³„ê°’ 0.95ë¡œ ìƒí–¥
    print("ğŸ—£ï¸ [2/5] Silero VAD ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ íƒì§€ ì¤‘...")
    
    SAMPLING_RATE = 16000 
    try:
        audio_tensor = read_audio(audio_path, sampling_rate=SAMPLING_RATE)
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return []

    speech_timestamps = get_speech_timestamps(
        audio_tensor, 
        model, 
        sampling_rate=SAMPLING_RATE,
        threshold=vad_threshold,
        min_speech_duration_ms=int(min_clip_duration * 1000), 
        min_silence_duration_ms=200 # ğŸ’¡ ì§§ì€ ì¹¨ë¬µì€ ì—°ê²° (í¥ë¶„ ìƒíƒœ ê³ ë ¤)
    )

    if not speech_timestamps:
        print("âš ï¸ VADë¡œ íƒì§€ëœ ìŒì„± êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    candidates = []
    for ts in speech_timestamps:
        start_s = ts['start'] / SAMPLING_RATE
        end_s = ts['end'] / SAMPLING_RATE
        
        if end_s - start_s >= min_clip_duration:
              candidates.append((start_s, end_s))
    
    print(f"âš½ íƒì§€ëœ í›„ë³´ êµ¬ê°„ ìˆ˜: {len(candidates)}ê°œ (VAD ì„ê³„ê°’: {vad_threshold}, ìµœì†Œ {min_clip_duration}s ì´ìƒ í•„í„°ë§)\n")
    return candidates


# =====================================================
# [3/5] í›„ë³´ êµ¬ê°„ ì˜¤ë””ì˜¤ ì¶”ì¶œ
# =====================================================
@timed
def extract_highlight_clips(audio_path, candidates):
    print("âœ‚ï¸ [3/5] í›„ë³´ êµ¬ê°„ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
    audio = AudioSegment.from_file(audio_path)
    clip_paths = []

    for i, (start, end) in enumerate(candidates):
        clip = audio[start * 1000 : end * 1000] 
        clip_path = os.path.join(OUTPUT_DIR, f"highlight_{i}.wav")
        clip.export(clip_path, format="wav", codec="pcm_s16le", parameters=["-ac", "1", "-ar", "16000"]) 
        clip_paths.append(clip_path)
        print(f"ğŸ—‚ï¸ êµ¬ê°„ {i+1}: {start:.1f}s ~ {end:.1f}s ({(end-start):.1f}s)")

    print(f"âœ… ì´ {len(clip_paths)}ê°œ í•˜ì´ë¼ì´íŠ¸ í´ë¦½ ìƒì„± ì™„ë£Œ\n")
    return clip_paths


# =====================================================
# [4/5] Google STT ë¹„ë™ê¸° ì²˜ë¦¬ (ë³‘ë ¬ì²˜ë¦¬ ë° íƒ€ì„ë¼ì¸ ìœ ì§€)
# =====================================================
@timed
def transcribe_candidates_async(clip_paths, candidates, bucket_name):
    print("ğŸ—£ï¸ [4/5] ë¹„ë™ê¸° STT ë³€í™˜ ì¤‘ (ë‹¤ì¤‘ í´ë¦½ ë™ì‹œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ )...")

    speech_client = speech.SpeechClient()
    
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="ko-KR",
        enable_automatic_punctuation=True,
        enable_word_time_offsets=True,
        speech_contexts=[
            speech.SpeechContext(
                phrases=["ê³¨", "ìŠ›", "ë“ì ", "ì„¸ì´ë¸Œ", "ì°¬ìŠ¤", "VAR", "í‚¤í¼", "í¬ë¡œìŠ¤", "í—¤ë”©"], 
                boost=15.0
            )
        ]
    )
    
    # 1. ëª¨ë“  í´ë¦½ì„ GCSì— ì—…ë¡œë“œí•˜ê³  ë¹„ë™ê¸° ìš”ì²­ ì‹œì‘
    operations = []
    gcs_files_info = []
    
    for i, (path, (clip_start_original, _)) in enumerate(zip(clip_paths, candidates)):
        gcs_blob_name = os.path.join(OUTPUT_DIR, os.path.basename(path))
        
        try:
            gcs_uri = upload_blob(bucket_name, path, gcs_blob_name)
            gcs_files_info.append((gcs_uri, clip_start_original)) 
            
            audio = speech.RecognitionAudio(uri=gcs_uri)
            operation = speech_client.long_running_recognize(config=config, audio=audio)
            operations.append(operation)
            print(f"   â–¶ï¸ ({i+1}/{len(clip_paths)}) GCS ì—…ë¡œë“œ ë° ë¹„ë™ê¸° ìš”ì²­ ì‹œì‘ ì™„ë£Œ: {os.path.basename(path)}")
        except Exception as e:
             print(f"   âŒ GCS ì—…ë¡œë“œ ë˜ëŠ” ë¹„ë™ê¸° ìš”ì²­ ì‹¤íŒ¨ ({os.path.basename(path)}): {e}")

    # 2. ëª¨ë“  ë¹„ë™ê¸° ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘
    final_transcripts = []
    
    for i, (operation, (gcs_uri, clip_start_original)) in enumerate(zip(operations, gcs_files_info)):
        print(f"\n   â³ ({i+1}/{len(operations)}) ê²°ê³¼ ëŒ€ê¸° ì¤‘: {os.path.basename(gcs_uri)}")
        
        try:
            response = operation.result(timeout=1000) 
        except Exception as e:
            print(f"   âŒ STT ì‘ì—… ì‹¤íŒ¨ ({os.path.basename(gcs_uri)}): {e}")
            continue

        # 3. ê²°ê³¼ íŒŒì‹± ë° ì›ë³¸ ì‹œê°„ ì˜¤í”„ì…‹ ì ìš© (íƒ€ì„ë¼ì¸ ìœ ì§€!)
        for result in response.results:
            words = result.alternatives[0].words
            
            if not words:
                continue
                
            start_time_s = words[0].start_time.total_seconds() 
            end_time_s = words[-1].end_time.total_seconds()
            text = result.alternatives[0].transcript
            
            # ğŸ’¡ ì›ë³¸ ì˜¤ë””ì˜¤ ê¸°ì¤€ ì‹œê°„ì„ ê³„ì‚°í•˜ì—¬ ì €ì¥
            final_transcripts.append((start_time_s + clip_start_original, end_time_s + clip_start_original, text))

    full_text = "\n".join([f"[{format_time(t[0])} ~ {format_time(t[1])}] {t[2]}" for t in final_transcripts])
    with open(os.path.join(OUTPUT_DIR, "transcript_for_llm.txt"), "w", encoding="utf-8") as f:
        f.write(full_text)

    print(f"\nâœ… ë¹„ë™ê¸° STT ë° í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ (LLMìš© íƒ€ì„ë¼ì¸ í¬í•¨)")
    return final_transcripts


# =====================================================
# [5/5] í‚¤ì›Œë“œ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (LLMìš©ì´ë¯€ë¡œ ë‹¨ìˆœ í•„í„°ë§)
# =====================================================
# ì´ ë‹¨ê³„ëŠ” LLMì—ê²Œ ë„˜ê¸°ê¸° ì „ì— ê°„ë‹¨íˆ ê²°ê³¼ë§Œ í™•ì¸í•˜ëŠ” ìš©ë„ì…ë‹ˆë‹¤.
@timed
def extract_keyword_highlights(final_transcripts, keywords=["ê³¨", "ìŠ›", "ë“ì ", "ì„¸ì´ë¸Œ", "ì°¬ìŠ¤"]):
    print("âš½ [5/5] í…ìŠ¤íŠ¸ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì¤‘...")

    highlights = []
    
    for start, end, text in final_transcripts:
        if not text:
            continue
            
        if any(k in text for k in keywords):
            highlights.append(f"[{format_time(start)} ~ {format_time(end)}] {text.strip()}")

    with open(os.path.join(OUTPUT_DIR, "highlight_result_filtered.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(highlights))

    print(f"âœ… í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ {len(highlights)}ê°œ ì¶”ì¶œ ì™„ë£Œ\n")
    return highlights


# =====================================================
# ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í´ë¦½ ê°œìˆ˜ ì œí•œ ì ìš©)
# =====================================================
if __name__ == "__main__":
    total_start = time.time()
    
    if "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
        print("\nğŸš¨ğŸš¨ğŸš¨ ì˜¤ë¥˜: GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Google STT/GCSë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸš¨ğŸš¨ğŸš¨")
    elif not os.path.exists(INPUT_AUDIO_FILE):
        print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ '{INPUT_AUDIO_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        vad_model, get_speech_timestamps, read_audio = load_silero_vad_model()
        
        if vad_model is None:
             print("\nğŸš« Silero VAD ë¡œë“œ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            cleaned = preprocess_audio(INPUT_AUDIO_FILE)
            
            # ğŸ’¡ VAD ê·¹ë‹¨ì  ìµœì í™” ì„¤ì •
            candidates = detect_highlight_candidates_vad(
                cleaned, 
                vad_model, 
                get_speech_timestamps, 
                read_audio, 
                vad_threshold=0.95, 
                min_clip_duration=3.0
            )
            
            if not candidates:
                print("\nğŸš¨ VAD íƒì§€ ê²°ê³¼, ìœ íš¨í•œ í›„ë³´ê°€ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            else:
                
                # --- ğŸ’¡ ì†ë„ ê°œì„  í•µì‹¬: STT ì…ë ¥ í´ë¦½ ê°œìˆ˜ ì œí•œ ---
                # í´ë¦½ ê¸¸ì´ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ê°€ì¥ ê¸´ í´ë¦½ì´ ë§¨ ìœ„ë¡œ)
                candidates.sort(key=lambda x: x[1] - x[0], reverse=True) 
                
                MAX_CLIPS_FOR_STT = 20 # ğŸ’¡ STTì— ë„˜ê¸¸ ìµœëŒ€ í´ë¦½ ê°œìˆ˜ (ì´ ìˆ«ìë¥¼ ì¡°ì •í•˜ì—¬ ì†ë„ ì¡°ì ˆ ê°€ëŠ¥)
                
                if len(candidates) > MAX_CLIPS_FOR_STT:
                    print(f"âš ï¸ íƒì§€ëœ {len(candidates)}ê°œ í´ë¦½ ì¤‘, STT ì²˜ë¦¬ ì†ë„ë¥¼ ìœ„í•´ ê°€ì¥ ê¸´ {MAX_CLIPS_FOR_STT}ê°œë§Œ ì„ ë³„í•©ë‹ˆë‹¤.")
                    candidates = candidates[:MAX_CLIPS_FOR_STT] 
                
                # --- ë ---

                clips = extract_highlight_clips(cleaned, candidates)
                
                # ë¹„ë™ê¸° STT í˜¸ì¶œ (ë³‘ë ¬ ì²˜ë¦¬)
                final_transcripts = transcribe_candidates_async(clips, candidates, bucket_name=GCS_BUCKET_NAME) 
                
                # LLMì—ê²Œ ë„˜ê¸°ê¸° ì „ì— í™•ì¸ìš© í•„í„°ë§
                extract_keyword_highlights(final_transcripts)

                print(f"\nğŸ¬ ì „ì²´ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time.time() - total_start:.2f}ì´ˆ ğŸ‰")