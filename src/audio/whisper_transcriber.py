"""
OpenAI Whisper ê¸°ë°˜ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜

CUDA, CPU ì§€ì› (MPSëŠ” í˜„ì¬ ë¯¸ì§€ì›)
"""

import whisper
import torch
import time
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import json


class WhisperTranscriber:
    """
    OpenAI Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜

    CUDA, CPU ìë™ ê°ì§€ ë° ìµœì í™”
    ì£¼ì˜: MPS(Apple Silicon)ëŠ” í˜„ì¬ Whisperì™€ í˜¸í™˜ì„± ë¬¸ì œë¡œ CPU ì‚¬ìš©

    Args:
        model_size: Whisper ëª¨ë¸ í¬ê¸°
            - tiny: 39M params (ê°€ì¥ ë¹ ë¦„, ë‚®ì€ ì •í™•ë„)
            - base: 74M params (ë¹ ë¦„, ì ì ˆí•œ ì •í™•ë„) â˜… ì¶”ì²œ
            - small: 244M params (ì¤‘ê°„ ì†ë„, ë†’ì€ ì •í™•ë„)
            - medium: 769M params (ëŠë¦¼, ë§¤ìš° ë†’ì€ ì •í™•ë„)
            - large: 1550M params (ë§¤ìš° ëŠë¦¼, ìµœê³  ì •í™•ë„)
        device: ë””ë°”ì´ìŠ¤ ('auto', 'cuda', 'cpu')
        language: ì–¸ì–´ ì½”ë“œ (None=ìë™ê°ì§€, 'ko'=í•œêµ­ì–´, 'en'=ì˜ì–´)
        verbose: ì§„í–‰ ìƒí™© ì¶œë ¥ ì—¬ë¶€
    """

    def __init__(
        self,
        model_size: str = "base",
        device: str = "auto",
        language: Optional[str] = None,
        verbose: bool = True
    ):
        self.model_size = model_size
        # "auto"ë¥¼ Noneìœ¼ë¡œ ë³€í™˜ (WhisperëŠ” Noneì¼ ë•Œ ìë™ ê°ì§€)
        self.language = None if language == "auto" else language
        self.verbose = verbose

        # ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€
        if device == "auto":
            # WhisperëŠ” í˜„ì¬ MPSë¥¼ ì™„ì „íˆ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ CPU ì‚¬ìš©
            if torch.cuda.is_available():
                self.device = "cuda"  # NVIDIA GPU
            else:
                self.device = "cpu"  # CPU (Apple Silicon í¬í•¨)
        else:
            self.device = device

        self._log(f"ğŸ”§ Whisper ì´ˆê¸°í™” ì¤‘...")
        self._log(f"   ëª¨ë¸: {model_size}")
        self._log(f"   ë””ë°”ì´ìŠ¤: {self.device}")

        # Whisper ëª¨ë¸ ë¡œë“œ
        load_start = time.time()
        self.model = whisper.load_model(model_size, device=self.device)
        load_time = time.time() - load_start

        self._log(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")

    def _log(self, message: str):
        """ì§„í–‰ ìƒí™© ë¡œê·¸ ì¶œë ¥"""
        if self.verbose:
            print(message)

    def transcribe(
        self,
        audio_path: str,
        segments: Optional[List[Tuple[float, float]]] = None
    ) -> Dict:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            segments: íŠ¹ì • êµ¬ê°„ë§Œ ë³€í™˜ [(start, end), ...] (Noneì´ë©´ ì „ì²´)

        Returns:
            Whisper ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            {
                'text': ì „ì²´ í…ìŠ¤íŠ¸,
                'segments': [
                    {
                        'start': ì‹œì‘ ì‹œê°„(ì´ˆ),
                        'end': ì¢…ë£Œ ì‹œê°„(ì´ˆ),
                        'text': í…ìŠ¤íŠ¸
                    },
                    ...
                ],
                'language': ê°ì§€ëœ ì–¸ì–´
            }
        """
        total_start = time.time()

        self._log("=" * 60)
        self._log("ğŸ™ï¸  ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ì‹œì‘")
        self._log("=" * 60)
        self._log(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {audio_path}")

        if segments:
            self._log(f"ğŸ¯ ì§€ì •ëœ êµ¬ê°„ ìˆ˜: {len(segments)}ê°œ")
            return self._transcribe_segments(audio_path, segments, total_start)
        else:
            self._log("ğŸ¯ ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜")
            return self._transcribe_full(audio_path, total_start)

    def _transcribe_full(
        self,
        audio_path: str,
        total_start: float
    ) -> Dict:
        """ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜"""
        self._log("\nğŸ”„ Whisper ë³€í™˜ ì¤‘...")
        self._log("   â³ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")

        transcribe_start = time.time()

        # Whisper ì‹¤í–‰
        result = self.model.transcribe(
            audio_path,
            language=self.language,
            verbose=False  # Whisper ìì²´ ë¡œê·¸ ë¹„í™œì„±í™”
        )

        transcribe_time = time.time() - transcribe_start
        total_time = time.time() - total_start

        # ê²°ê³¼ ì¶œë ¥
        self._log(f"\nâœ… ë³€í™˜ ì™„ë£Œ ({transcribe_time:.2f}ì´ˆ)")
        self._log(f"   ğŸŒ ê°ì§€ëœ ì–¸ì–´: {result.get('language', 'unknown')}")
        self._log(f"   ğŸ“ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(result['segments'])}ê°œ")
        self._log(f"   ğŸ“„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])}ì")

        self._log("\n" + "=" * 60)
        self._log(f"âœ¨ ì „ì²´ ì‘ì—… ì™„ë£Œ! (ì´ {total_time:.2f}ì´ˆ)")
        self._log("=" * 60)

        return result

    def _transcribe_segments(
        self,
        audio_path: str,
        segments: List[Tuple[float, float]],
        total_start: float
    ) -> Dict:
        """
        íŠ¹ì • êµ¬ê°„ë§Œ ë³€í™˜ (í•˜ì´ë¼ì´íŠ¸ í•„í„°ë§ í›„)

        FFmpegë¡œ ì„ì‹œ íŒŒì¼ì„ ë§Œë“¤ì§€ ì•Šê³  Whisperì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ëŠ¥ í™œìš©
        """
        self._log("\nğŸ”„ [1/2] ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜ ì¤‘...")
        self._log("   â³ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")

        transcribe_start = time.time()

        # ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        result = self.model.transcribe(
            audio_path,
            language=self.language,
            verbose=False
        )

        transcribe_time = time.time() - transcribe_start
        self._log(f"âœ… [1/2] ë³€í™˜ ì™„ë£Œ ({transcribe_time:.2f}ì´ˆ)")
        self._log(f"   ğŸ“ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜: {len(result['segments'])}ê°œ")

        # ì§€ì •ëœ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ë§Œ í•„í„°ë§
        self._log("\nğŸ”„ [2/2] í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ í•„í„°ë§ ì¤‘...")
        filter_start = time.time()

        filtered_segments = []
        for whisper_seg in result['segments']:
            seg_start = whisper_seg['start']
            seg_end = whisper_seg['end']

            # ì´ ì„¸ê·¸ë¨¼íŠ¸ê°€ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ê³¼ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            for highlight_start, highlight_end in segments:
                # ê²¹ì¹¨ ì—¬ë¶€ í™•ì¸
                if not (seg_end < highlight_start or seg_start > highlight_end):
                    filtered_segments.append(whisper_seg)
                    break

        # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        filtered_text = " ".join(seg['text'].strip() for seg in filtered_segments)

        filter_time = time.time() - filter_start
        total_time = time.time() - total_start

        self._log(f"âœ… [2/2] í•„í„°ë§ ì™„ë£Œ ({filter_time:.2f}ì´ˆ)")
        self._log(f"   ğŸ“ í•„í„°ë§ëœ ì„¸ê·¸ë¨¼íŠ¸: {len(filtered_segments)}ê°œ")
        self._log(f"   ğŸ“„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(filtered_text)}ì")

        reduction_rate = (1 - len(filtered_segments) / len(result['segments'])) * 100
        self._log(f"\nğŸ“Š í•„í„°ë§ ê²°ê³¼:")
        self._log(f"   ğŸ“‰ ì„¸ê·¸ë¨¼íŠ¸ ê°ì†Œìœ¨: {reduction_rate:.1f}%")

        self._log("\n" + "=" * 60)
        self._log(f"âœ¨ ì „ì²´ ì‘ì—… ì™„ë£Œ! (ì´ {total_time:.2f}ì´ˆ)")
        self._log("=" * 60)

        # í•„í„°ë§ëœ ê²°ê³¼ ë°˜í™˜
        return {
            'text': filtered_text,
            'segments': filtered_segments,
            'language': result.get('language', 'unknown'),
            'original_segments_count': len(result['segments']),
            'filtered_segments_count': len(filtered_segments)
        }

    def save_transcript(
        self,
        result: Dict,
        output_path: str,
        format: str = "txt"
    ):
        """
        ë³€í™˜ ê²°ê³¼ ì €ì¥

        Args:
            result: transcribe() ê²°ê³¼
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            format: ì €ì¥ í˜•ì‹ ('txt', 'json', 'srt')
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        if format == "txt":
            # í…ìŠ¤íŠ¸ íŒŒì¼
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(result['text'])

        elif format == "json":
            # JSON íŒŒì¼ (ì „ì²´ ì •ë³´ í¬í•¨)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

        elif format == "srt":
            # SRT ìë§‰ íŒŒì¼
            with open(output_file, 'w', encoding='utf-8') as f:
                for i, seg in enumerate(result['segments'], 1):
                    start_time = self._format_timestamp(seg['start'])
                    end_time = self._format_timestamp(seg['end'])
                    text = seg['text'].strip()

                    f.write(f"{i}\n")
                    f.write(f"{start_time} --> {end_time}\n")
                    f.write(f"{text}\n\n")

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")

        self._log(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_file}")

    def _format_timestamp(self, seconds: float) -> str:
        """SRT íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (00:00:00,000)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)

        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    import sys

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python whisper_transcriber.py <audio_file> [output.txt]")
        sys.exit(1)

    audio_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else "output/transcript.txt"

    # Whisper ë³€í™˜
    transcriber = WhisperTranscriber(
        model_size="base",
        device="auto",
        language="ko"
    )

    result = transcriber.transcribe(audio_path)

    # ê²°ê³¼ ì €ì¥
    transcriber.save_transcript(result, output_path, format="txt")
    transcriber.save_transcript(
        result,
        output_path.replace('.txt', '.json'),
        format="json"
    )

    print(f"\nğŸ“„ ë³€í™˜ëœ í…ìŠ¤íŠ¸:")
    print(result['text'][:500])  # ì²˜ìŒ 500ìë§Œ ì¶œë ¥
    if len(result['text']) > 500:
        print("...")
