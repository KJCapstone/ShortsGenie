"""
ì˜¤ë””ì˜¤ í•˜ì´ë¼ì´íŠ¸ í•„í„°ë§ ë° STT ë³€í™˜ ì˜ˆì œ

Phase 1 êµ¬í˜„: librosa + Whisper
- RMS ì—ë„ˆì§€ ë° Spectral ë¶„ì„ìœ¼ë¡œ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ ì¶”ì¶œ
- Whisperë¡œ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ë§Œ STT ë³€í™˜
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.audio import AudioHighlightFilter, WhisperTranscriber


def example_1_audio_filtering():
    """
    ì˜ˆì œ 1: ì˜¤ë””ì˜¤ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ ì¶”ì¶œ

    RMS ì—ë„ˆì§€ì™€ Spectral ë¶„ì„ì„ ê²°í•©í•˜ì—¬
    ê²½ê¸° í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ì„ ìë™ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
    """
    print("=" * 70)
    print("ì˜ˆì œ 1: ì˜¤ë””ì˜¤ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ ì¶”ì¶œ")
    print("=" * 70)

    # ì…ë ¥ íŒŒì¼ (ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”)
    audio_path = "input/sample_match.mp3"

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(audio_path).exists():
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        print(f"ğŸ’¡ 'input/' ë””ë ‰í† ë¦¬ì— ì˜¤ë””ì˜¤ íŒŒì¼ì„ ë„£ê³  ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
        return None

    # í•˜ì´ë¼ì´íŠ¸ í•„í„° ìƒì„±
    audio_filter = AudioHighlightFilter(
        threshold_percentile=60,  # ìƒìœ„ 40% êµ¬ê°„ ìœ ì§€
        merge_gap=2.0,            # 2ì´ˆ ì´ë‚´ êµ¬ê°„ ë³‘í•©
        segment_duration=5.0,     # ê° êµ¬ê°„ 5ì´ˆ
        rms_weight=0.7,           # RMS 70%
        spectral_weight=0.3,      # Spectral 30%
        verbose=True
    )

    # í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ ì¶”ì¶œ
    segments = audio_filter.filter_audio(audio_path)

    print(f"\nğŸ“‹ ì¶”ì¶œëœ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„:")
    for i, (start, end) in enumerate(segments, 1):
        print(f"   {i}. {start:.1f}ì´ˆ ~ {end:.1f}ì´ˆ (ê¸¸ì´: {end-start:.1f}ì´ˆ)")

    return segments


def example_2_whisper_full():
    """
    ì˜ˆì œ 2: Whisper ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜

    ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì²´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    (ë¹„êµë¥¼ ìœ„í•œ ì˜ˆì œ)
    """
    print("\n" + "=" * 70)
    print("ì˜ˆì œ 2: Whisper ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜ (ë¹„êµìš©)")
    print("=" * 70)

    # ì…ë ¥ íŒŒì¼
    audio_path = "input/sample_match.mp3"

    if not Path(audio_path).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        return

    # Whisper ë³€í™˜ê¸° ìƒì„±
    transcriber = WhisperTranscriber(
        model_size="base",   # base ëª¨ë¸ ì‚¬ìš©
        device="auto",       # ìë™ ë””ë°”ì´ìŠ¤ ì„ íƒ
        language="ko",       # í•œêµ­ì–´
        verbose=True
    )

    # ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜
    result = transcriber.transcribe(audio_path)

    # ê²°ê³¼ ì €ì¥
    output_path = "output/transcript_full.txt"
    transcriber.save_transcript(result, output_path, format="txt")
    transcriber.save_transcript(
        result,
        output_path.replace('.txt', '.json'),
        format="json"
    )

    print(f"\nğŸ“„ ë³€í™˜ëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì):")
    print(result['text'][:500])
    if len(result['text']) > 500:
        print("...")


def example_3_combined_pipeline():
    """
    ì˜ˆì œ 3: í†µí•© íŒŒì´í”„ë¼ì¸

    1. ì˜¤ë””ì˜¤ í•˜ì´ë¼ì´íŠ¸ í•„í„°ë§
    2. í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ë§Œ STT ë³€í™˜

    â†’ ì „ì²´ ì‹œê°„ ëŒ€ë¹„ 60% ê°ì†Œ!
    """
    print("\n" + "=" * 70)
    print("ì˜ˆì œ 3: í†µí•© íŒŒì´í”„ë¼ì¸ (í•„í„°ë§ + STT)")
    print("=" * 70)

    # ì…ë ¥ íŒŒì¼
    audio_path = "input/sample_match.mp3"

    if not Path(audio_path).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        return

    # Step 1: í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ ì¶”ì¶œ
    print("\nğŸ” Step 1: í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ ì¶”ì¶œ")
    print("-" * 70)

    audio_filter = AudioHighlightFilter(
        threshold_percentile=60,
        merge_gap=2.0,
        segment_duration=5.0,
        verbose=True
    )

    segments = audio_filter.filter_audio(audio_path)

    if not segments:
        print("âŒ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    # Step 2: í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ë§Œ STT ë³€í™˜
    print("\nğŸ™ï¸ Step 2: í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ STT ë³€í™˜")
    print("-" * 70)

    transcriber = WhisperTranscriber(
        model_size="base",
        device="auto",
        language="ko",
        verbose=True
    )

    result = transcriber.transcribe(audio_path, segments=segments)

    # ê²°ê³¼ ì €ì¥
    output_path = "output/transcript_filtered.txt"
    transcriber.save_transcript(result, output_path, format="txt")
    transcriber.save_transcript(
        result,
        output_path.replace('.txt', '.json'),
        format="json"
    )

    print(f"\nğŸ“„ ë³€í™˜ëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì):")
    print(result['text'][:500])
    if len(result['text']) > 500:
        print("...")

    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
    print(f"   ì›ë³¸ ì„¸ê·¸ë¨¼íŠ¸: {result.get('original_segments_count', 'N/A')}ê°œ")
    print(f"   í•„í„°ë§ í›„: {result.get('filtered_segments_count', 'N/A')}ê°œ")

    if result.get('original_segments_count'):
        reduction = (1 - result['filtered_segments_count'] / result['original_segments_count']) * 100
        print(f"   ê°ì†Œìœ¨: {reduction:.1f}%")


def example_4_custom_config():
    """
    ì˜ˆì œ 4: ì‚¬ìš©ì ì„¤ì • ì¡°ì •

    ìƒí™©ì— ë§ê²Œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    print("\n" + "=" * 70)
    print("ì˜ˆì œ 4: ì‚¬ìš©ì ì„¤ì • ì¡°ì •")
    print("=" * 70)

    audio_path = "input/sample_match.mp3"

    if not Path(audio_path).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_path}")
        return

    # ì„¤ì • 1: ê³µê²©ì ì¸ í•„í„°ë§ (80% ê°ì†Œ)
    print("\nğŸ”¥ ì„¤ì • 1: ê³µê²©ì ì¸ í•„í„°ë§ (ìƒìœ„ 20%ë§Œ ìœ ì§€)")
    print("-" * 70)

    aggressive_filter = AudioHighlightFilter(
        threshold_percentile=80,  # ìƒìœ„ 20%ë§Œ ìœ ì§€
        merge_gap=1.0,            # 1ì´ˆ ì´ë‚´ë§Œ ë³‘í•©
        segment_duration=3.0,     # ì§§ì€ êµ¬ê°„ (3ì´ˆ)
        rms_weight=0.8,           # ìŒëŸ‰ ìš°ì„ 
        spectral_weight=0.2,
        verbose=True
    )

    segments = aggressive_filter.filter_audio(audio_path)
    print(f"   ì¶”ì¶œëœ êµ¬ê°„: {len(segments)}ê°œ")

    # ì„¤ì • 2: ë³´ìˆ˜ì ì¸ í•„í„°ë§ (40% ê°ì†Œ)
    print("\nğŸ›¡ï¸ ì„¤ì • 2: ë³´ìˆ˜ì ì¸ í•„í„°ë§ (ìƒìœ„ 60% ìœ ì§€)")
    print("-" * 70)

    conservative_filter = AudioHighlightFilter(
        threshold_percentile=40,  # ìƒìœ„ 60% ìœ ì§€
        merge_gap=3.0,            # 3ì´ˆ ì´ë‚´ ë³‘í•©
        segment_duration=7.0,     # ê¸´ êµ¬ê°„ (7ì´ˆ)
        rms_weight=0.5,           # ê· í˜•
        spectral_weight=0.5,
        verbose=True
    )

    segments = conservative_filter.filter_audio(audio_path)
    print(f"   ì¶”ì¶œëœ êµ¬ê°„: {len(segments)}ê°œ")


if __name__ == "__main__":
    print("ğŸµ ì˜¤ë””ì˜¤ í•˜ì´ë¼ì´íŠ¸ í•„í„°ë§ ë° STT ì˜ˆì œ")
    print("=" * 70)
    print()
    print("ğŸ“Œ ì¤€ë¹„ì‚¬í•­:")
    print("   1. 'input/' ë””ë ‰í† ë¦¬ì— ì˜¤ë””ì˜¤ íŒŒì¼ ë„£ê¸° (sample_match.mp3)")
    print("   2. librosa, soundfile, openai-whisper ì„¤ì¹˜ í™•ì¸")
    print("   3. Apple Silicon ì‚¬ìš© ì‹œ MPS ê°€ì† ìë™ í™œì„±í™”")
    print()

    # ì˜ˆì œ ì„ íƒ
    print("ì‹¤í–‰í•  ì˜ˆì œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("  1. ì˜¤ë””ì˜¤ í•˜ì´ë¼ì´íŠ¸ êµ¬ê°„ ì¶”ì¶œë§Œ")
    print("  2. Whisper ì „ì²´ ì˜¤ë””ì˜¤ ë³€í™˜ (ë¹„êµìš©)")
    print("  3. í†µí•© íŒŒì´í”„ë¼ì¸ (í•„í„°ë§ + STT) â˜… ì¶”ì²œ")
    print("  4. ì‚¬ìš©ì ì„¤ì • ì¡°ì • ì˜ˆì œ")
    print("  0. ëª¨ë‘ ì‹¤í–‰")

    choice = input("\nì„ íƒ (0-4): ").strip()

    if choice == "1":
        example_1_audio_filtering()
    elif choice == "2":
        example_2_whisper_full()
    elif choice == "3":
        example_3_combined_pipeline()
    elif choice == "4":
        example_4_custom_config()
    elif choice == "0":
        example_1_audio_filtering()
        example_2_whisper_full()
        example_3_combined_pipeline()
        example_4_custom_config()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

    print("\n" + "=" * 70)
    print("âœ¨ ì˜ˆì œ ì‹¤í–‰ ì™„ë£Œ!")
    print("=" * 70)
