import os
import ffmpeg
import librosa
import numpy as np
from faster_whisper import WhisperModel
import torch
import soundfile as sf
import time

# ---------- ì‚¬ìš©ì ì„¤ì • ----------
VIDEO_PATH = "match.mp4"
AUDIO_FILE_WAV = "temp_audio.wav"
CHUNK_DIR = "librosa_chunks"
OUTPUT_TXT = "script.txt"

MODEL_FAST = "tiny"   # ë¹ ë¥¸ ìŠ¤ìº”ìš©
MODEL_FULL = "base"   # ìµœì¢… ë³€í™˜ìš©
LANG = "ko"

RMS_THRESHOLD = 0.02       # ë°œí™” íŒë‹¨ ê¸°ì¤€
MIN_CHUNK_LEN = 1.0         # ìµœì†Œ ì²­í¬ ê¸¸ì´ (ì´ˆ)
MAX_CHUNK_LEN = 30.0        # ìµœëŒ€ ì²­í¬ ê¸¸ì´ (ì´ˆ)
KEYWORDS = ["ê³¨", "ì¶•êµ¬", "ë“ì "]  # ê´€ì‹¬ í‚¤ì›Œë“œ

# ---------- í•¨ìˆ˜ ì •ì˜ ----------

# 1ï¸âƒ£ ì˜¤ë””ì˜¤ ì¶”ì¶œ
def extract_audio(video_path, audio_path):
    print(f"ğŸ§ ì˜¤ë””ì˜¤ ì¶”ì¶œ: {video_path}")
    ffmpeg_path = r"C:\ffmpeg\bin\ffmpeg.exe"  # ì ˆëŒ€ê²½ë¡œ ì§€ì •

    (
        ffmpeg
        .input(video_path)
        .output(audio_path, ac=1, ar=16000, acodec="pcm_s16le")
        .overwrite_output()
        .run(cmd=ffmpeg_path, quiet=True)
    )
    print(f"âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ â†’ {audio_path}")

# 2ï¸âƒ£ RMS ê¸°ë°˜ ë°œí™” êµ¬ê°„ ì¶”ì¶œ
def extract_speech_chunks(audio_path, chunk_dir,
                          rms_thresh=RMS_THRESHOLD,
                          min_len=MIN_CHUNK_LEN,
                          max_len=MAX_CHUNK_LEN):
    print(f"\nğŸ”‡ RMS ê¸°ë°˜ ë°œí™” êµ¬ê°„ ì¶”ì¶œ: {audio_path}")
    y, sr = librosa.load(audio_path, sr=16000)
    hop_length = 512
    frame_length = 1024
    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]

    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=hop_length)
    speech_mask = rms > rms_thresh

    # ì—°ì† êµ¬ê°„ ë³‘í•©
    chunks = []
    start = None
    for t, is_speech in zip(times, speech_mask):
        if is_speech and start is None:
            start = t
        elif not is_speech and start is not None:
            end = t
            if end - start >= min_len:
                chunks.append((start, min(end, start+max_len)))
            start = None
    if start is not None:
        chunks.append((start, min(times[-1], start+max_len)))

    # íŒŒì¼ë¡œ ì €ì¥
    os.makedirs(chunk_dir, exist_ok=True)
    chunk_paths = []
    for i, (start, end) in enumerate(chunks):
        path = os.path.join(chunk_dir, f"chunk_{i}.wav")
        sf.write(path, y[int(start*sr):int(end*sr)], sr)
        chunk_paths.append((path, start, end))

    print(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± â†’ {chunk_dir}")
    return chunk_paths

# 3ï¸âƒ£ ë¹ ë¥¸ ëª¨ë¸ë¡œ í‚¤ì›Œë“œ í¬í•¨ ì²­í¬ ì„ íƒ
def filter_chunks_by_keyword(chunk_paths, model_size=MODEL_FAST, lang=LANG, keywords=KEYWORDS):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device=="cuda" else "int8"
    model = WhisperModel(model_size, device=device, compute_type=compute_type)
    
    selected_chunks = []
    for path, start_s, end_s in chunk_paths:
        segments, _ = model.transcribe(path, language=lang)
        text = " ".join(seg.text for seg in segments).lower()
        if any(k.lower() in text for k in keywords):
            selected_chunks.append((path, start_s, end_s))
    print(f"âœ… {len(selected_chunks)}/{len(chunk_paths)}ê°œ ì²­í¬ê°€ í‚¤ì›Œë“œ í¬í•¨")
    return selected_chunks

# 4ï¸âƒ£ ìµœì¢… ë³€í™˜ + íƒ€ì„ìŠ¤íƒ¬í”„
def transcribe_final(chunks, output_txt, model_size=MODEL_FULL, lang=LANG):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device=="cuda" else "int8"
    model = WhisperModel(model_size, device=device, compute_type=compute_type)

    start_all = time.perf_counter()
    with open(output_txt, "w", encoding="utf-8") as f_out:
        f_out.write("===== [Transcript with Timestamps] =====\n")
        for i, (path, start_s, _) in enumerate(chunks):
            chunk_start = time.perf_counter()
            print(f"â–¶ï¸ ë³€í™˜ ì¤‘: ì²­í¬ {i+1}/{len(chunks)} â†’ {path}")
            segments, _ = model.transcribe(path, language=lang)
            for seg in segments:
                seg_start = start_s + seg.start
                seg_end = start_s + seg.end
                line = f"[{seg_start:.1f}~{seg_end:.1f}] {seg.text.strip()}\n"
                f_out.write(line)
            elapsed = time.perf_counter() - chunk_start
            print(f"âœ… ì²­í¬ {i+1} ì™„ë£Œ ({elapsed:.1f}s)")
    print(f"\nâœ… ì „ì²´ ë³€í™˜ ì™„ë£Œ â†’ {os.path.abspath(output_txt)}")
    print(f"ì´ ì†Œìš” ì‹œê°„: {time.perf_counter()-start_all:.1f}s")

# ----------------- ì‹¤í–‰ -----------------
if __name__ == "__main__":
    extract_audio(VIDEO_PATH, AUDIO_FILE_WAV)
    chunks = extract_speech_chunks(AUDIO_FILE_WAV, CHUNK_DIR)
    selected_chunks = filter_chunks_by_keyword(chunks)
    transcribe_final(selected_chunks, OUTPUT_TXT)

    # ì„ì‹œ ì˜¤ë””ì˜¤ ì œê±°
    if os.path.exists(AUDIO_FILE_WAV):
        os.remove(AUDIO_FILE_WAV)
