import os
import ffmpeg
from faster_whisper import WhisperModel
import torch
import time
import librosa
import soundfile as sf
import shutil # ì„ì‹œ í´ë” ì‚­ì œë¥¼ ìœ„í•´ ì¶”ê°€

# ---------- ì‚¬ìš©ì ì„¤ì • ----------
VIDEO_PATH = "match.mp4"
AUDIO_FILE_WAV = "temp_audio.wav"
CLEAN_AUDIO_DIR = "clean_chunks"
OUTPUT_TXT = "script.txt"

MODEL_SIZE = "tiny"   # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: "tiny", "base", "small", "medium", "large-v3"
LANG = "ko"           # ì–¸ì–´ ì½”ë“œ (e.g., "ko", "en", "ja")

# librosa.effects.splitì„ ìœ„í•œ ì„¤ì •ê°’
# ìŒëŸ‰ì´ ê°€ì¥ í° ë¶€ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ top_db ë°ì‹œë²¨ ë§Œí¼ ë‚®ì€ ì†Œë¦¬ê¹Œì§€ë¥¼ ìœ íš¨í•œ ì†Œë¦¬ë¡œ ê°„ì£¼
# ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¹¨ë¬µì„ ë” ì—„ê²©í•˜ê²Œ íŒë‹¨í•˜ì—¬ ë” ì˜ê²Œ ìª¼ê°¬
TOP_DB = 35
# ----------------------------------


def extract_audio(video_path, audio_path):
    """ffmpegë¡œ ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (16kHz, ëª¨ë…¸, PCM)"""
    print(f"ğŸ§ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹œì‘: {video_path}")
    try:
        (
            ffmpeg
            .input(video_path)
            .output(audio_path, ac=1, ar=16000, acodec="pcm_s16le")
            .overwrite_output()
            .run(quiet=True)
        )
    except ffmpeg.Error as e:
        print("ffmpeg ì˜¤ë¥˜ ë°œìƒ:")
        print(e.stderr.decode())
        raise


def fast_split_audio_with_librosa(audio_path, output_dir, top_db):
    """librosaë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ ë¹ ë¥´ê²Œ ë¹„ìŒì„± êµ¬ê°„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• """
    print(f"ğŸ”‡ librosaë¡œ ì²­í¬ ë¶„ë¦¬ ì‹œì‘: {audio_path}")
    
    y, sr = librosa.load(audio_path, sr=16000)
    intervals = librosa.effects.split(y, top_db=top_db)

    if len(intervals) == 0:
        raise Exception("ë°œí™” êµ¬ê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. TOP_DB ê°’ì„ ì¡°ì ˆí•´ë³´ì„¸ìš”.")

    os.makedirs(output_dir, exist_ok=True)
    chunk_paths = []
    chunk_times_in_audio = []

    for i, (start_frame, end_frame) in enumerate(intervals):
        chunk = y[start_frame:end_frame]
        path = os.path.join(output_dir, f"chunk_{i}.wav")
        sf.write(path, chunk, sr)
        chunk_paths.append(path)

        start_s = librosa.frames_to_time(start_frame, sr=sr)
        end_s = librosa.frames_to_time(end_frame, sr=sr)
        chunk_times_in_audio.append((start_s, end_s))

    print(f"âœ… ì´ {len(intervals)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ â†’ {output_dir}")
    return chunk_paths, chunk_times_in_audio


def transcribe_chunks_merge_text(chunk_paths, chunk_times, output_txt, model_size, lang):
    """ì²­í¬ë³„ Whisper ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ë³‘í•© ë° íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device == "cuda" else "int8"

    print(f"\nğŸ¤ Whisper ëª¨ë¸ ë¡œë“œ ì‹œì‘: {model_size} (Device: {device}, Compute Type: {compute_type})")
    model_load_start = time.perf_counter()
    model = WhisperModel(model_size, device=device, compute_type=compute_type)
    model_load_elapsed = time.perf_counter() - model_load_start
    print(f"âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {model_load_elapsed:.2f}ì´ˆ)")
    
    transcribe_start_time = time.perf_counter()

    with open(output_txt, "w", encoding="utf-8") as f_out:
        f_out.write(f"===== [ìŒì„±ì¸ì‹ ìŠ¤í¬ë¦½íŠ¸] ì›ë³¸: {VIDEO_PATH} =====\n\n")

        for i, path in enumerate(chunk_paths):
            print(f"â–¶ï¸ ë³€í™˜ ì¤‘: ì²­í¬ {i + 1}/{len(chunk_paths)} ({path})")
            
            segments, _ = model.transcribe(path, beam_size=5, language=lang)
            start_s_in_video, _ = chunk_times[i]

            for seg in segments:
                text = seg.text.strip()
                if text:
                    seg_start = start_s_in_video + seg.start
                    seg_end = start_s_in_video + seg.end
                    line = f"[{seg_start:0>7.2f} ~ {seg_end:0>7.2f}] {text}\n"
                    f_out.write(line)

    transcribe_elapsed = time.perf_counter() - transcribe_start_time
    print(f"âœ… ëª¨ë“  ì²­í¬ ë³€í™˜ ì™„ë£Œ (ìŒì„± ì¸ì‹ ì†Œìš” ì‹œê°„: {transcribe_elapsed:.2f}ì´ˆ)")
    print(f"ê²°ê³¼ íŒŒì¼ì´ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.abspath(output_txt)}")


if __name__ == "__main__":
    total_start_time = time.perf_counter()
    
    try:
        # --- 1ë‹¨ê³„: ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ---
        step1_start_time = time.perf_counter()
        extract_audio(VIDEO_PATH, AUDIO_FILE_WAV)
        step1_elapsed = time.perf_counter() - step1_start_time
        print(f"âœ… 1ë‹¨ê³„: ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {step1_elapsed:.2f}ì´ˆ)")

        # --- 2ë‹¨ê³„: ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ ---
        step2_start_time = time.perf_counter()
        chunk_paths, chunk_times = fast_split_audio_with_librosa(AUDIO_FILE_WAV, CLEAN_AUDIO_DIR, top_db=TOP_DB)
        step2_elapsed = time.perf_counter() - step2_start_time
        print(f"âœ… 2ë‹¨ê³„: ì˜¤ë””ì˜¤ ì²­í¬ ë¶„ë¦¬ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {step2_elapsed:.2f}ì´ˆ)")

        # --- 3ë‹¨ê³„: ì²­í¬ë³„ ìŒì„± ì¸ì‹ ë° ê²°ê³¼ ë³‘í•© ---
        step3_start_time = time.perf_counter()
        transcribe_chunks_merge_text(chunk_paths, chunk_times, OUTPUT_TXT, MODEL_SIZE, LANG)
        step3_elapsed = time.perf_counter() - step3_start_time
        print(f"âœ… 3ë‹¨ê³„: ìŒì„± ì¸ì‹ ë° ë³‘í•© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {step3_elapsed:.2f}ì´ˆ)")

    except Exception as e:
        print(f"\nâ—ï¸ì˜¤ë¥˜ ë°œìƒ: {e}")

    finally:
        # --- 4ë‹¨ê³„: ì„ì‹œ íŒŒì¼ ë° í´ë” ì •ë¦¬ ---
        print("\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        try:
            if os.path.exists(AUDIO_FILE_WAV):
                os.remove(AUDIO_FILE_WAV)
            if os.path.exists(CLEAN_AUDIO_DIR):
                shutil.rmtree(CLEAN_AUDIO_DIR)
            print("âœ… ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ.")
        except OSError as e:
            print(f"â—ï¸ì„ì‹œ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    total_elapsed = time.perf_counter() - total_start_time
    print(f"\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.2f}ì´ˆ)")