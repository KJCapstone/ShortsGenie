import os
import io
import time
import numpy as np
import matplotlib.pyplot as plt
from pydub import AudioSegment, silence
from google.cloud import speech_v1p1beta1 as speech
# ğŸ’¡ Silero VADë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ 
import torch
import torchaudio
from scipy.ndimage.measurements import label as scipy_label 


# =====================================================
# âœ… ì„¤ì • (ìˆ˜ì •ë¨)
# =====================================================
INPUT_AUDIO_FILE = "match_audio.wav"
OUTPUT_DIR = "highlight_v2"

# âŒ ë³´ì•ˆ ì´ìŠˆë¡œ ì¸í•´ í•˜ë“œì½”ë”©ëœ GOOGLE_CREDENTIALS ê²½ë¡œëŠ” ì‚­ì œí•©ë‹ˆë‹¤.
# GOOGLE_CREDENTIALS = r"C:\Users\home\Desktop\shortsgenie\ShortsGenie\src\videototext\diesel-channel-477619-u6-db0de75fbe60.json" 

# âœ… ì½”ë“œëŠ” ì´ì œ ì‹¤í–‰ ì „ì— ì‚¬ìš©ìê°€ í™˜ê²½ ë³€ìˆ˜ 'GOOGLE_APPLICATION_CREDENTIALS'ë¥¼
# âœ… ì§ì ‘ ì„¤ì •í–ˆìŒì„ ê°€ì •í•˜ê³  ì§„í–‰í•©ë‹ˆë‹¤.

# í˜„ì¬ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë‹¤ìŒ ì½”ë“œëŠ” ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ë¯€ë¡œ,
# ì„ì‹œì ìœ¼ë¡œ ì½”ë“œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½í•˜ì—¬ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
if "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
    # ğŸš¨ğŸš¨ğŸš¨ ì´ ì¤„ì€ ì„ì‹œ ì½”ë“œì´ë©°, ì‹¤ì œë¡œëŠ” í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‰˜ì—ì„œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. ğŸš¨ğŸš¨ğŸš¨
    print("\nâš ï¸ ê²½ê³ : GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    # ê°œë°œ í¸ì˜ë¥¼ ìœ„í•´ ì„¤ì • íŒŒì¼ ê²½ë¡œë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •í•´ì•¼ í•œë‹¤ë©´, ì´ ë¶€ë¶„ì„ ì£¼ì„ í•´ì œí•˜ê³  ì‚¬ìš©í•˜ì„¸ìš”.
    # ë‹¨, ì´ íŒŒì¼ì€ ë°˜ë“œì‹œ .gitignoreì— ì¶”ê°€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    # os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = r"C:\Users\home\Desktop\shortsgenie\ShortsGenie\src\videototext\diesel-channel-477619-u6-db0de75fbe60.json" 
    pass


os.makedirs(OUTPUT_DIR, exist_ok=True)


# =====================================================
# âœ… ê³µìš© ìœ í‹¸: ì²˜ë¦¬ì‹œê°„ ì¸¡ì • & íƒ€ì„ í¬ë§·
# (ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼)
# =====================================================
def timed(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"âœ… {func.__name__} ì™„ë£Œ (ê±¸ë¦° ì‹œê°„: {end - start:.2f}ì´ˆ)\n")
        return result
    return wrapper

# ì´ˆë¥¼ HH:MM:SS.msms í¬ë§·ìœ¼ë¡œ ë³€í™˜
def format_time(seconds):
    """ì´ˆë¥¼ HH:MM:SS.msms í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    # ì†Œìˆ˜ì  ë‘˜ì§¸ ìë¦¬ê¹Œì§€ í‘œì‹œ (10ms ë‹¨ìœ„)
    ms = int((seconds - int(seconds)) * 100)
    s = int(seconds)
    m = s // 60
    h = m // 60
    return f"{h:02d}:{m % 60:02d}:{s % 60:02d}.{ms:02d}"

# =====================================================
# ğŸ’¡ Silero VAD ìœ í‹¸ë¦¬í‹°
# =====================================================
@timed
def load_silero_vad_model():
    """Silero VAD ëª¨ë¸ ë¡œë“œ"""
    print("ğŸ§  Silero VAD ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )
        (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
        return model, get_speech_timestamps, read_audio
    except Exception as e:
        print(f"âŒ Silero VAD ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (torch, torchaudio) ì„¤ì¹˜ ë° ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}")
        return None, None, None


# =====================================================
# [1/5] ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
# =====================================================
@timed
def preprocess_audio(input_path):
    print("ğŸ§ [1/5] ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì¤‘...")
    audio = AudioSegment.from_file(input_path)
    original_len = len(audio) / 1000

    # âš ï¸ VADëŠ” ìŒì„±ë§Œì„ ê²€ì¶œí•˜ë¯€ë¡œ, ì „ì²˜ë¦¬ì—ì„œ ê³¼ë„í•œ Silence ì œê±°ëŠ” ì‹ ì¤‘í•´ì•¼ í•¨
    # ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ 16kHz, ëª¨ë…¸ë¡œ ë³€í™˜ ë° ì •ê·œí™”ë§Œ ìˆ˜í–‰
    cleaned = audio.set_frame_rate(16000).set_channels(1)
    cleaned = cleaned.apply_gain(-20 - cleaned.dBFS) # ëŒ€ëµ -20dBFSë¡œ ì •ê·œí™”

    output_path = os.path.join(OUTPUT_DIR, "cleaned_audio.wav")
    cleaned.export(output_path, format="wav", codec="pcm_s16le")

    print(f"ğŸ“¦ ì›ë³¸ê¸¸ì´: {original_len:.1f}s â†’ ì „ì²˜ë¦¬ í›„: {len(cleaned)/1000:.1f}s\n")
    return output_path


# =====================================================
# [2/5] Silero VAD ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ íƒì§€ 
# =====================================================
@timed
def detect_highlight_candidates_vad(audio_path, model, get_speech_timestamps, read_audio, 
                                    vad_threshold=0.8, min_clip_duration=2.0):
    print("ğŸ—£ï¸ [2/5] Silero VAD ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ íƒì§€ ì¤‘...")
    
    # 1. ì˜¤ë””ì˜¤ ë¡œë“œ ë° í…ì„œ ë³€í™˜ (Silero VAD ìš”êµ¬ì‚¬í•­)
    SAMPLING_RATE = 16000 # ëª¨ë¸ì´ 16000Hzë¥¼ ìš”êµ¬
    try:
        # VAD ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” í…ì„œ í˜•íƒœë¡œ ë¡œë“œ
        audio_tensor = read_audio(audio_path, sampling_rate=SAMPLING_RATE)
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return []

    # 2. ìŒì„± í™œë™ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
    speech_timestamps = get_speech_timestamps(
        audio_tensor, 
        model, 
        sampling_rate=SAMPLING_RATE,
        threshold=vad_threshold,
        min_speech_duration_ms=int(min_clip_duration * 1000), 
        min_silence_duration_ms=400 # ì§§ì€ ì¹¨ë¬µì€ ì—°ê²°
    )

    if not speech_timestamps:
        print("âš ï¸ VADë¡œ íƒì§€ëœ ìŒì„± êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # 3. ìƒ˜í”Œ ì¸ë±ìŠ¤ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
    candidates = []
    for ts in speech_timestamps:
        start_s = ts['start'] / SAMPLING_RATE
        end_s = ts['end'] / SAMPLING_RATE
        
        if end_s - start_s >= min_clip_duration:
             candidates.append((start_s, end_s))
    
    print(f"âš½ íƒì§€ëœ í›„ë³´ êµ¬ê°„ ìˆ˜: {len(candidates)}ê°œ (VAD ì„ê³„ê°’: {vad_threshold}, ìµœì†Œ {min_clip_duration}s ì´ìƒ í•„í„°ë§)\n")
    return candidates


# =====================================================
# [3/5] í›„ë³´ êµ¬ê°„ ì˜¤ë””ì˜¤ ì¶”ì¶œ
# =====================================================
@timed
def extract_highlight_clips(audio_path, candidates):
    print("âœ‚ï¸ [3/5] í›„ë³´ êµ¬ê°„ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
    audio = AudioSegment.from_file(audio_path)
    clip_paths = []

    for i, (start, end) in enumerate(candidates):
        clip = audio[start * 1000 : end * 1000]
        clip_path = os.path.join(OUTPUT_DIR, f"highlight_{i}.wav")
        clip.export(clip_path, format="wav", codec="pcm_s16le", parameters=["-ac", "1", "-ar", "16000"])
        clip_paths.append(clip_path)
        print(f"ğŸ—‚ï¸ êµ¬ê°„ {i+1}: {start:.1f}s ~ {end:.1f}s ({(end-start):.1f}s)")

    print(f"âœ… ì´ {len(clip_paths)}ê°œ í•˜ì´ë¼ì´íŠ¸ í´ë¦½ ìƒì„± ì™„ë£Œ\n")
    return clip_paths


# =====================================================
# [4/5] Google STT (í›„ë³´ êµ¬ê°„ë§Œ)
# =====================================================
def stt_google_sdk(audio_path):
    """Google STT (10MB/60ì´ˆ ì´í•˜ë§Œ)"""
    client = speech.SpeechClient()

    with io.open(audio_path, "rb") as f:
        audio_content = f.read()

    audio = speech.RecognitionAudio(content=audio_content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="ko-KR",
        enable_automatic_punctuation=True,
        enable_word_time_offsets=True,
        speech_contexts=[
            speech.SpeechContext(
                phrases=[
                    "ê³¨", "ìŠ›", "ë“ì ", "ì„¸ì´ë¸Œ", "ì°¬ìŠ¤", "íŒ¨ìŠ¤", "ë“œë¦¬ë¸”",
                    "í”„ë¦¬í‚¥", "ì½”ë„ˆí‚¥", "ì˜¤í”„ì‚¬ì´ë“œ", "íŒ¨ë„í‹°í‚¥", "VAR",
                    "í‚¤í¼", "ìˆ˜ë¹„", "ê³µê²©", "í¬ë¡œìŠ¤", "ì‹¬íŒ", "í—¤ë”©"
                ],
                boost=15.0 
            )
        ]
    )

    response = client.recognize(config=config, audio=audio)
    
    sentence_data = []
    
    for result in response.results:
        words = result.alternatives[0].words
        
        if not words:
            continue
            
        start_time_s = words[0].start_time.total_seconds() 
        end_time_s = words[-1].end_time.total_seconds()
        
        text = result.alternatives[0].transcript
        
        sentence_data.append((start_time_s, end_time_s, text))
        
    return sentence_data


@timed
def transcribe_candidates(clip_paths, candidates):
    print("ğŸ—£ï¸ [4/5] í›„ë³´ êµ¬ê°„ STT ë³€í™˜ ì¤‘ (STT 60ì´ˆ ì œí•œ ëŒ€ì‘ ìë™ ë¶„í• )...")
    
    final_transcripts = [] 

    for i, (path, (clip_start_original, _)) in enumerate(zip(clip_paths, candidates)):
        print(f"\nğŸ§ ({i+1}/{len(clip_paths)}) ë³€í™˜ ì¤‘: {os.path.basename(path)}")

        audio = AudioSegment.from_file(path)
        duration_ms = len(audio)

        MAX_DURATION_MS = 58 * 1000 
        
        temp_sentence_data = []

        if duration_ms > MAX_DURATION_MS:
            print(f"âš ï¸ í´ë¦½ ê¸¸ì´ {duration_ms/1000:.1f}s (60ì´ˆ ì´ˆê³¼) â†’ STT ì œí•œì— ë§ì¶° {MAX_DURATION_MS/1000:.1f}ì´ˆ ë‹¨ìœ„ë¡œ ìë™ ë¶„í•  ì‹¤í–‰")

            start_ms = 0
            part_num = 1
            while start_ms < duration_ms:
                end_ms = min(start_ms + MAX_DURATION_MS, duration_ms)
                part = audio[start_ms:end_ms]
                temp_path = f"{path[:-4]}_part{part_num}.wav"
                
                part.export(temp_path, format="wav", codec="pcm_s16le", parameters=["-ac", "1", "-ar", "16000"])
                print(f" Â â†³ íŒŒíŠ¸ {part_num}: {start_ms/1000:.1f}s~{end_ms/1000:.1f}s ({(end_ms-start_ms)/1000:.1f}s)")
                
                try:
                    part_sentences = stt_google_sdk(temp_path)
                    
                    offset_s = start_ms / 1000.0
                    for s_start, s_end, s_text in part_sentences:
                        temp_sentence_data.append((s_start + offset_s, s_end + offset_s, s_text))
                        
                except Exception as e:
                    print(f" Â âŒ íŒŒíŠ¸ {part_num} ì‹¤íŒ¨: {e}")
                os.remove(temp_path)
                start_ms = end_ms
                part_num += 1
                
        else:
            try:
                temp_sentence_data = stt_google_sdk(path)
            except Exception as e:
                print(f" Â âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        
        offset_original_s = clip_start_original
        for s_start, s_end, s_text in temp_sentence_data:
            final_transcripts.append((s_start + offset_original_s, s_end + offset_original_s, s_text))


    full_text = "\n".join([t[2] for t in final_transcripts])
    with open(os.path.join(OUTPUT_DIR, "transcript_candidates.txt"), "w", encoding="utf-8") as f:
        f.write(full_text)

    print(f"âœ… í›„ë³´ êµ¬ê°„ í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ\n")
    return final_transcripts 


# =====================================================
# [5/5] í‚¤ì›Œë“œ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# =====================================================
@timed
def extract_keyword_highlights(final_transcripts, keywords=["ê³¨", "ìŠ›", "ë“ì ", "ì„¸ì´ë¸Œ", "ì°¬ìŠ¤"]):
    print("âš½ [5/5] í…ìŠ¤íŠ¸ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì¤‘...")

    highlights = []
    
    for start, end, text in final_transcripts:
        if not text:
            continue
            
        if any(k in text for k in keywords):
            highlights.append(f"[{format_time(start)} ~ {format_time(end)}] {text.strip()}")

    with open(os.path.join(OUTPUT_DIR, "highlight_result.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(highlights))

    print(f"âœ… í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ {len(highlights)}ê°œ ì¶”ì¶œ ì™„ë£Œ\n")
    return highlights


# =====================================================
# ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# =====================================================
if __name__ == "__main__":
    total_start = time.time()
    
    # ğŸ’¡ VAD ëª¨ë¸ ë¡œë“œ
    vad_model, get_speech_timestamps, read_audio = load_silero_vad_model()
    
    if vad_model is None or not os.path.exists(INPUT_AUDIO_FILE):
        if not os.path.exists(INPUT_AUDIO_FILE):
             print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ '{INPUT_AUDIO_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        # VAD ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œì—ë„ ì¢…ë£Œ
        print("\nğŸš« Silero VAD ë¡œë“œ ë˜ëŠ” íŒŒì¼ ì˜¤ë¥˜ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    # í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ê²½ê³  í›„ ì¢…ë£Œ (ì„ íƒ ì‚¬í•­: ì‹¤í–‰ì„ ë§‰ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.)
    elif "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
        print("\nğŸš¨ğŸš¨ğŸš¨ ì˜¤ë¥˜: GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Google STTë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸš¨ğŸš¨ğŸš¨")

    else:
        cleaned = preprocess_audio(INPUT_AUDIO_FILE)
        
        # ğŸ’¡ RMS ëŒ€ì‹  VAD í•¨ìˆ˜ í˜¸ì¶œ
        candidates = detect_highlight_candidates_vad(
            cleaned, 
            vad_model, 
            get_speech_timestamps, 
            read_audio, 
            vad_threshold=0.8, # VAD ì„ê³„ê°’ (0.1 ~ 0.99)
            min_clip_duration=2.0
        )
        
        if not candidates:
            print("\nğŸš¨ VAD íƒì§€ ê²°ê³¼, ìœ íš¨í•œ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ê°€ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        else:
            clips = extract_highlight_clips(cleaned, candidates)
            
            final_transcripts = transcribe_candidates(clips, candidates)
            
            extract_keyword_highlights(final_transcripts)

            print(f"\nğŸ¬ ì „ì²´ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time.time() - total_start:.2f}ì´ˆ ğŸ‰")