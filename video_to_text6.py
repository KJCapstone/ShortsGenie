import os
import ffmpeg
from faster_whisper import WhisperModel
import torch
import time
from pydub import AudioSegment, effects
from pydub.silence import split_on_silence

# ---------- ì‚¬ìš©ì ì„¤ì • ----------
VIDEO_PATH = "match.mp4"
AUDIO_FILE_WAV = "temp_audio.wav"
CLEAN_AUDIO_DIR = "clean_chunks"
OUTPUT_TXT = "match_transcript.txt"

MODEL_SIZE = "tiny"   # tiny/base
LANG = "ko"

MIN_SILENCE_LEN = 1000
SILENCE_THRESH = -35
KEEP_SILENCE = 500
# ----------------------------------


def extract_audio(video_path, audio_path):
    """ffmpegë¡œ ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ (16kHz, ëª¨ë…¸, PCM)"""
    print(f"ğŸ§ ì˜¤ë””ì˜¤ ì¶”ì¶œ: {video_path}")
    (
        ffmpeg
        .input(video_path)
        .output(audio_path, ac=1, ar=16000, acodec="pcm_s16le")
        .overwrite_output()
        .run(quiet=False)
    )
    print(f"âœ… ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ â†’ {audio_path}")


def split_and_clean_audio(audio_path, output_dir,
                          min_silence_len, silence_thresh, keep_silence):
    """ì˜¤ë””ì˜¤ë¥¼ ì¹¨ë¬µ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ë¶„ë¦¬ í›„ normalize"""
    print(f"\nğŸ”‡ ì¹¨ë¬µ ê¸°ì¤€ ì²­í¬ ë¶„ë¦¬: {audio_path}")
    audio = AudioSegment.from_wav(audio_path)
    audio = effects.normalize(audio)

    avg_db = audio.dBFS
    adjusted_thresh = silence_thresh if avg_db < silence_thresh else avg_db - 10
    print(f"í‰ê·  ë³¼ë¥¨: {avg_db:.1f} dBFS â†’ ì„ê³„ê°’: {adjusted_thresh:.1f} dBFS")

    chunks = split_on_silence(audio,
                              min_silence_len=min_silence_len,
                              silence_thresh=adjusted_thresh,
                              keep_silence=keep_silence)

    if not chunks:
        raise Exception("âŒ ë°œí™” êµ¬ê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. SILENCE_THRESH ì¡°ì • í•„ìš”")

    os.makedirs(output_dir, exist_ok=True)
    chunk_paths = []
    chunk_times_in_audio = []

    start_ms = 0
    for i, chunk in enumerate(chunks):
        path = os.path.join(output_dir, f"chunk_{i}.wav")
        chunk.export(path, format="wav")
        chunk_paths.append(path)
        # ì²­í¬ ì‹œì‘/ë ì‹œê°„ ê¸°ë¡ (ms â†’ s)
        end_ms = start_ms + len(chunk)
        chunk_times_in_audio.append((start_ms/1000.0, end_ms/1000.0))
        start_ms = end_ms

    print(f"âœ… ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„± â†’ {output_dir}")
    return chunk_paths, chunk_times_in_audio


def transcribe_chunks_merge_text(chunk_paths, chunk_times, output_txt, model_size, lang):
    """ì²­í¬ë³„ Whisper ë³€í™˜ í›„ í…ìŠ¤íŠ¸ ë³‘í•© + ì˜ìƒ íƒ€ì„ìŠ¤íƒ¬í”„ + ì²­í¬ ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device == "cuda" else "int8"

    print(f"\nâš™ï¸ Whisper ëª¨ë¸ ë¡œë“œ: {model_size}, {device}, {compute_type}")
    model = WhisperModel(model_size, device=device, compute_type=compute_type)

    all_text = []
    start_time_all = time.perf_counter()

    with open(output_txt, "w", encoding="utf-8") as f_out:
        f_out.write(f"===== [Transcript with Timestamps] Source: {os.path.basename(chunk_paths[0])} =====\n")

        for i, path in enumerate(chunk_paths):
            chunk_start_time = time.perf_counter()
            start_s, end_s = chunk_times[i]
            print(f"â–¶ï¸ ë³€í™˜ ì¤‘: ì²­í¬ {i + 1}/{len(chunk_paths)} â†’ {path}")
            
            segments, _ = model.transcribe(path, beam_size=5, language=lang)

            chunk_elapsed = time.perf_counter() - chunk_start_time
            print(f"âœ… ì²­í¬ {i + 1} ì™„ë£Œ (ì†Œìš”: {chunk_elapsed:.1f}s)")

            for seg in segments:
                text = seg.text.strip()
                if text:
                    # ì˜ìƒ ê¸°ì¤€ íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
                    seg_start = start_s + seg.start
                    seg_end = start_s + seg.end
                    line = f"[{seg_start:.1f}~{seg_end:.1f}] {text}\n"
                    f_out.write(line)
                    all_text.append(text)

    total_elapsed = time.perf_counter() - start_time_all
    print(f"\nâœ… ì „ì²´ ë³€í™˜ ì™„ë£Œ â†’ {os.path.abspath(output_txt)}")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_elapsed:.1f}s")


if __name__ == "__main__":
    try:
        # 1ï¸âƒ£ ì˜¤ë””ì˜¤ ì¶”ì¶œ
        extract_audio(VIDEO_PATH, AUDIO_FILE_WAV)

        # 2ï¸âƒ£ ì²­í¬ ë‹¨ìœ„ ë¶„ë¦¬
        chunk_paths, chunk_times = split_and_clean_audio(AUDIO_FILE_WAV, CLEAN_AUDIO_DIR,
                                                         MIN_SILENCE_LEN, SILENCE_THRESH, KEEP_SILENCE)

        # 3ï¸âƒ£ ì²­í¬ë³„ ë³€í™˜ + ì˜ìƒ íƒ€ì„ìŠ¤íƒ¬í”„ + ì²­í¬ ì²˜ë¦¬ ì‹œê°„
        transcribe_chunks_merge_text(chunk_paths, chunk_times, OUTPUT_TXT, MODEL_SIZE, LANG)

    except Exception as e:
        print(f"\nğŸš¨ ì˜¤ë¥˜ ë°œìƒ: {e}")

    finally:
        # 4ï¸âƒ£ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            if os.path.exists(AUDIO_FILE_WAV):
                os.remove(AUDIO_FILE_WAV)
        except OSError as e:
            print(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")
