import os
import io
import time
import numpy as np
import matplotlib.pyplot as plt
from pydub import AudioSegment
from google.cloud import speech_v1p1beta1 as speech
from google.cloud import storage # ğŸ’¡ GCS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import torch
import torchaudio
from scipy.ndimage.measurements import label as scipy_label 


# =====================================================
# âœ… ì„¤ì • (ìˆ˜ì •ë¨)
# =====================================================
INPUT_AUDIO_FILE = "match_audio.wav"
OUTPUT_DIR = "highlight_v2"

# âš ï¸ GCS ë²„í‚· ì´ë¦„: ì‹¤ì œ ì‚¬ìš©ìì˜ ë²„í‚· ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”!
GCS_BUCKET_NAME = "shortsgenie-voice" 

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)


# =====================================================
# âœ… ê³µìš© ìœ í‹¸: ì²˜ë¦¬ì‹œê°„ ì¸¡ì • & íƒ€ì„ í¬ë§·
# =====================================================
def timed(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"âœ… {func.__name__} ì™„ë£Œ (ê±¸ë¦° ì‹œê°„: {end - start:.2f}ì´ˆ)\n")
        return result
    return wrapper

# ì´ˆë¥¼ HH:MM:SS.msms í¬ë§·ìœ¼ë¡œ ë³€í™˜
def format_time(seconds):
    """ì´ˆë¥¼ HH:MM:SS.msms í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
    ms = int((seconds - int(seconds)) * 100)
    s = int(seconds)
    m = s // 60
    h = m // 60
    return f"{h:02d}:{m % 60:02d}:{s % 60:02d}.{ms:02d}"


# =====================================================
# ğŸ’¡ GCS ìœ í‹¸ë¦¬í‹° (ë¹„ë™ê¸° STT í•„ìˆ˜)
# =====================================================
def upload_blob(bucket_name, source_file_name, destination_blob_name):
    """ë¡œì»¬ íŒŒì¼ì„ GCSì— ì—…ë¡œë“œ"""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)

    # ë¡œì»¬ íŒŒì¼ì„ GCSë¡œ ì—…ë¡œë“œ
    blob.upload_from_filename(source_file_name)
    
    # GCS URI í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
    return f"gs://{bucket_name}/{destination_blob_name}"


# =====================================================
# ğŸ’¡ Silero VAD ìœ í‹¸ë¦¬í‹°
# =====================================================
@timed
def load_silero_vad_model():
    """Silero VAD ëª¨ë¸ ë¡œë“œ"""
    print("ğŸ§  Silero VAD ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        model, utils = torch.hub.load(
            repo_or_dir='snakers4/silero-vad',
            model='silero_vad',
            force_reload=False
        )
        (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
        return model, get_speech_timestamps, read_audio
    except Exception as e:
        print(f"âŒ Silero VAD ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì˜¤ë¥˜: {e}")
        return None, None, None


# =====================================================
# [1/5] ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬
# =====================================================
@timed
def preprocess_audio(input_path):
    print("ğŸ§ [1/5] ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ì¤‘...")
    audio = AudioSegment.from_file(input_path)
    original_len = len(audio) / 1000

    # ì›ë³¸ ì˜¤ë””ì˜¤ë¥¼ 16kHz, ëª¨ë…¸ë¡œ ë³€í™˜ ë° ì •ê·œí™”
    cleaned = audio.set_frame_rate(16000).set_channels(1)
    cleaned = cleaned.apply_gain(-20 - cleaned.dBFS) # ëŒ€ëµ -20dBFSë¡œ ì •ê·œí™”

    output_path = os.path.join(OUTPUT_DIR, "cleaned_audio.wav")
    cleaned.export(output_path, format="wav", codec="pcm_s16le")

    print(f"ğŸ“¦ ì›ë³¸ê¸¸ì´: {original_len:.1f}s â†’ ì „ì²˜ë¦¬ í›„: {len(cleaned)/1000:.1f}s\n")
    return output_path


# =====================================================
# [2/5] Silero VAD ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ íƒì§€ (ìµœì í™” ì ìš©)
# =====================================================
@timed
def detect_highlight_candidates_vad(audio_path, model, get_speech_timestamps, read_audio, 
                                    vad_threshold=0.9, min_clip_duration=3.0): # ğŸ’¡ ì„ê³„ê°’ ìƒí–¥
    print("ğŸ—£ï¸ [2/5] Silero VAD ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ íƒì§€ ì¤‘...")
    
    SAMPLING_RATE = 16000 
    try:
        audio_tensor = read_audio(audio_path, sampling_rate=SAMPLING_RATE)
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ í…ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return []

    # ìŒì„± í™œë™ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
    speech_timestamps = get_speech_timestamps(
        audio_tensor, 
        model, 
        sampling_rate=SAMPLING_RATE,
        threshold=vad_threshold,
        min_speech_duration_ms=int(min_clip_duration * 1000), 
        min_silence_duration_ms=500 # ğŸ’¡ ì§§ì€ ì¹¨ë¬µì€ ì—°ê²°
    )

    if not speech_timestamps:
        print("âš ï¸ VADë¡œ íƒì§€ëœ ìŒì„± êµ¬ê°„ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # ìƒ˜í”Œ ì¸ë±ìŠ¤ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ ë° í•„í„°ë§
    candidates = []
    for ts in speech_timestamps:
        start_s = ts['start'] / SAMPLING_RATE
        end_s = ts['end'] / SAMPLING_RATE
        
        if end_s - start_s >= min_clip_duration:
              candidates.append((start_s, end_s))
    
    print(f"âš½ íƒì§€ëœ í›„ë³´ êµ¬ê°„ ìˆ˜: {len(candidates)}ê°œ (VAD ì„ê³„ê°’: {vad_threshold}, ìµœì†Œ {min_clip_duration}s ì´ìƒ í•„í„°ë§)\n")
    return candidates


# =====================================================
# [3/5] í›„ë³´ êµ¬ê°„ ì˜¤ë””ì˜¤ ì¶”ì¶œ
# =====================================================
@timed
def extract_highlight_clips(audio_path, candidates):
    print("âœ‚ï¸ [3/5] í›„ë³´ êµ¬ê°„ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...")
    audio = AudioSegment.from_file(audio_path)
    clip_paths = []

    for i, (start, end) in enumerate(candidates):
        # pydubëŠ” ms ë‹¨ìœ„
        clip = audio[start * 1000 : end * 1000] 
        clip_path = os.path.join(OUTPUT_DIR, f"highlight_{i}.wav")
        # STTì— ì í•©í•˜ë„ë¡ í¬ë§· ì„¤ì •
        clip.export(clip_path, format="wav", codec="pcm_s16le", parameters=["-ac", "1", "-ar", "16000"]) 
        clip_paths.append(clip_path)
        print(f"ğŸ—‚ï¸ êµ¬ê°„ {i+1}: {start:.1f}s ~ {end:.1f}s ({(end-start):.1f}s)")

    print(f"âœ… ì´ {len(clip_paths)}ê°œ í•˜ì´ë¼ì´íŠ¸ í´ë¦½ ìƒì„± ì™„ë£Œ\n")
    return clip_paths


# =====================================================
# [4/5] Google STT ë¹„ë™ê¸° ì²˜ë¦¬ (ì†ë„ ê°œì„ )
# =====================================================
@timed
def transcribe_candidates_async(clip_paths, candidates, bucket_name):
    print("ğŸ—£ï¸ [4/5] ë¹„ë™ê¸° STT ë³€í™˜ ì¤‘ (ë‹¤ì¤‘ í´ë¦½ ë™ì‹œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ )...")

    speech_client = speech.SpeechClient()
    
    # 60ì´ˆ ì œí•œì„ ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ì–´ì§
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="ko-KR",
        enable_automatic_punctuation=True,
        enable_word_time_offsets=True,
        speech_contexts=[
            speech.SpeechContext(
                phrases=["ê³¨", "ìŠ›", "ë“ì ", "ì„¸ì´ë¸Œ", "ì°¬ìŠ¤", "íŒ¨ìŠ¤", "ë“œë¦¬ë¸”", "í”„ë¦¬í‚¥", "ì½”ë„ˆí‚¥", "ì˜¤í”„ì‚¬ì´ë“œ", "íŒ¨ë„í‹°í‚¥", "VAR"], 
                boost=15.0
            )
        ]
    )
    
    # 1. ëª¨ë“  í´ë¦½ì„ GCSì— ì—…ë¡œë“œí•˜ê³  ë¹„ë™ê¸° ìš”ì²­ ì‹œì‘
    operations = []
    gcs_files_info = []
    
    for i, (path, (clip_start_original, _)) in enumerate(zip(clip_paths, candidates)):
        # GCS ë‚´ ê°ì²´ ì´ë¦„ ì„¤ì • (ì˜ˆ: highlight_v2/highlight_0.wav)
        gcs_blob_name = os.path.join(OUTPUT_DIR, os.path.basename(path))
        
        # 1-1. GCSì— ì—…ë¡œë“œ
        try:
            gcs_uri = upload_blob(bucket_name, path, gcs_blob_name)
            gcs_files_info.append((gcs_uri, clip_start_original)) 
            
            # 1-2. ë¹„ë™ê¸° STT ìš”ì²­
            audio = speech.RecognitionAudio(uri=gcs_uri)
            operation = speech_client.long_running_recognize(config=config, audio=audio)
            operations.append(operation)
            print(f"   â–¶ï¸ ({i+1}/{len(clip_paths)}) GCS ì—…ë¡œë“œ ë° ë¹„ë™ê¸° ìš”ì²­ ì‹œì‘ ì™„ë£Œ: {os.path.basename(path)}")
        except Exception as e:
             print(f"   âŒ GCS ì—…ë¡œë“œ ë˜ëŠ” ë¹„ë™ê¸° ìš”ì²­ ì‹¤íŒ¨ ({os.path.basename(path)}): {e}")

    # 2. ëª¨ë“  ë¹„ë™ê¸° ì‘ì—… ê²°ê³¼ ìˆ˜ì§‘
    final_transcripts = []
    
    for i, (operation, (gcs_uri, clip_start_original)) in enumerate(zip(operations, gcs_files_info)):
        print(f"\n   â³ ({i+1}/{len(operations)}) ê²°ê³¼ ëŒ€ê¸° ì¤‘: {os.path.basename(gcs_uri)}")
        
        try:
            # ë³‘ë ¬ ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ë„ì°©í•  ë•Œê¹Œì§€ ëŒ€ê¸°
            response = operation.result(timeout=1000) 
        except Exception as e:
            print(f"   âŒ STT ì‘ì—… ì‹¤íŒ¨ ({os.path.basename(gcs_uri)}): {e}")
            continue

        # 3. ê²°ê³¼ íŒŒì‹± ë° ì›ë³¸ ì‹œê°„ ì˜¤í”„ì…‹ ì ìš©
        for result in response.results:
            words = result.alternatives[0].words
            
            if not words:
                continue
                
            start_time_s = words[0].start_time.total_seconds() 
            end_time_s = words[-1].end_time.total_seconds()
            text = result.alternatives[0].transcript
            
            # íƒ€ì„ë¼ì¸ì„ ìœ„í•´ ì›ë³¸ ì˜¤ë””ì˜¤ ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ ë³€í™˜
            final_transcripts.append((start_time_s + clip_start_original, end_time_s + clip_start_original, text))

    full_text = "\n".join([t[2] for t in final_transcripts])
    with open(os.path.join(OUTPUT_DIR, "transcript_candidates_async.txt"), "w", encoding="utf-8") as f:
        f.write(full_text)

    print(f"\nâœ… ë¹„ë™ê¸° STT ë° í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ")
    return final_transcripts


# =====================================================
# [5/5] í‚¤ì›Œë“œ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
# =====================================================
@timed
def extract_keyword_highlights(final_transcripts, keywords=["ê³¨", "ìŠ›", "ë“ì ", "ì„¸ì´ë¸Œ", "ì°¬ìŠ¤", "ëŒ€ë°•", "í™˜ìƒì ì¸", "ë¯¸ì³¤ë‹¤"]):
    print("âš½ [5/5] í…ìŠ¤íŠ¸ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ ì¤‘...")

    highlights = []
    
    for start, end, text in final_transcripts:
        if not text:
            continue
            
        # í‚¤ì›Œë“œ íƒì§€
        if any(k in text for k in keywords):
            # ë¬¸ì¥ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ ê²ƒì€ ì œì™¸ (ë‹¨ì–´ ìˆ˜ 3ê°œ ë¯¸ë§Œ)
            if len(text.strip().split()) >= 3:
                highlights.append(f"[{format_time(start)} ~ {format_time(end)}] {text.strip()}")

    with open(os.path.join(OUTPUT_DIR, "highlight_result.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(highlights))

    print(f"âœ… í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸ {len(highlights)}ê°œ ì¶”ì¶œ ì™„ë£Œ\n")
    return highlights


# =====================================================
# ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# =====================================================
if __name__ == "__main__":
    total_start = time.time()
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í™•ì¸ (STT ë° GCS ì ‘ê·¼ í•„ìˆ˜)
    if "GOOGLE_APPLICATION_CREDENTIALS" not in os.environ:
        print("\nğŸš¨ğŸš¨ğŸš¨ ì˜¤ë¥˜: GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Google STT/GCSë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸš¨ğŸš¨ğŸš¨")
    elif not os.path.exists(INPUT_AUDIO_FILE):
        print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ '{INPUT_AUDIO_FILE}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        # ğŸ’¡ VAD ëª¨ë¸ ë¡œë“œ
        vad_model, get_speech_timestamps, read_audio = load_silero_vad_model()
        
        if vad_model is None:
             print("\nğŸš« Silero VAD ë¡œë“œ ì‹¤íŒ¨ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            cleaned = preprocess_audio(INPUT_AUDIO_FILE)
            
            # VAD ìµœì í™” ì„¤ì •ìœ¼ë¡œ í›„ë³´ íƒì§€
            candidates = detect_highlight_candidates_vad(
                cleaned, 
                vad_model, 
                get_speech_timestamps, 
                read_audio, 
                vad_threshold=0.9, 
                min_clip_duration=3.0
            )
            
            if not candidates:
                print("\nğŸš¨ VAD íƒì§€ ê²°ê³¼, ìœ íš¨í•œ í•˜ì´ë¼ì´íŠ¸ í›„ë³´ê°€ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            else:
                clips = extract_highlight_clips(cleaned, candidates)
                
                # ğŸ’¡ ë¹„ë™ê¸° STT í˜¸ì¶œ (ì†ë„ ê°œì„  í•µì‹¬)
                final_transcripts = transcribe_candidates_async(clips, candidates, bucket_name=GCS_BUCKET_NAME) 
                
                extract_keyword_highlights(final_transcripts)

                print(f"\nğŸ¬ ì „ì²´ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {time.time() - total_start:.2f}ì´ˆ ğŸ‰")